{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Installing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loading and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek: veriyi okuma\n",
    "df = pd.read_excel(\"Tweets.xlsx\")\n",
    "\n",
    "df = df[df[\"Target\"] != \"Donald Trump\"]\n",
    "\n",
    "# Kolon isimlerinin aynen kaldığını varsayıyoruz:\n",
    "# [\"Tweet\", \"Target\", \"Train/Test\", \"Stance\", \"Opinion Toward\", \"Sentiment labels\"]\n",
    "\n",
    "def basic_preprocessing(text):\n",
    "    # Küçük harfe çevir\n",
    "    text = text.lower()\n",
    "    # @ ile başlayan kelimeleri temizle\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Özel karakterleri temizle, sadece harf, rakam, boşluk, '-', '_', ve '#' karakterlerini bırak\n",
    "    text = re.sub(r\"[^\\w\\s#_-]+\", \" \", text)\n",
    "    # Fazla boşlukları sil\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()  # Boşlukları (trim) temizle\n",
    "    return text\n",
    "\n",
    "df[\"Preprocessed_Tweet\"] = df[\"Tweet\"].apply(basic_preprocessing)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Editing Category (Target) Classes and Stance (FAVOR / AGAINST / NEITHER) Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target için LabelEncoder\n",
    "le_target = LabelEncoder()\n",
    "df[\"Target_label\"] = le_target.fit_transform(df[\"Target\"])\n",
    "\n",
    "# Örnek olarak \"Atheism=0, Hillary Clinton=1, Feminist Mov=2, ...\" gibi dönüştürmüş olacak.\n",
    "num_classes_target = len(df[\"Target_label\"].unique())\n",
    "\n",
    "\n",
    "# Stance için LabelEncoder\n",
    "le_stance = LabelEncoder()\n",
    "df[\"Stance_label\"] = le_stance.fit_transform(df[\"Stance\"])\n",
    "num_classes_stance = len(df[\"Stance_label\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training/Validation Spliting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df[\"Train/Test\"] == \"Train\"].copy()\n",
    "df_test  = df[df[\"Train/Test\"] == \"Test\"].copy()\n",
    "\n",
    "X_train_text = df_train[\"Preprocessed_Tweet\"].values\n",
    "y_train_target = df_train[\"Target_label\"].values\n",
    "y_train_stance = df_train[\"Stance_label\"].values\n",
    "\n",
    "X_test_text = df_test[\"Preprocessed_Tweet\"].values\n",
    "y_test_target = df_test[\"Target_label\"].values\n",
    "y_test_stance = df_test[\"Stance_label\"].values\n",
    "\n",
    "\n",
    "X_tr_text, X_val_text, y_tr_target, y_val_target = train_test_split(\n",
    "    X_train_text, y_train_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_tr_text_s, X_val_text_s, y_tr_stance, y_val_stance = train_test_split(\n",
    "    X_train_text, y_train_stance, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenize and Sequence Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000   # Sözlük boyutu (isteğe göre artırılabilir)\n",
    "MAX_SEQ_LEN = 30         # Maksimum token sayısı\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_train[\"Preprocessed_Tweet\"])\n",
    "\n",
    "def text_to_seq(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
    "    return padded\n",
    "\n",
    "X_tr_seq = text_to_seq(X_tr_text)\n",
    "X_val_seq = text_to_seq(X_val_text)\n",
    "X_test_seq = text_to_seq(X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model–1: Target Classification - Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\telat\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_target = keras.models.Sequential([\n",
    "    layers.Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=128, \n",
    "                     input_length=MAX_SEQ_LEN),\n",
    "    layers.LSTM(128, return_sequences=False),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes_target, activation='softmax')\n",
    "])\n",
    "\n",
    "model_target.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Traninig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.2388 - loss: 1.5902 - val_accuracy: 0.3499 - val_loss: 1.4151\n",
      "Epoch 2/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4021 - loss: 1.2883 - val_accuracy: 0.3739 - val_loss: 1.3121\n",
      "Epoch 3/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4631 - loss: 1.0535 - val_accuracy: 0.3962 - val_loss: 1.3513\n",
      "Epoch 4/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.5335 - loss: 0.9067 - val_accuracy: 0.4734 - val_loss: 1.5173\n",
      "Epoch 5/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.7359 - loss: 0.6239 - val_accuracy: 0.6158 - val_loss: 1.1487\n",
      "Epoch 6/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8479 - loss: 0.4096 - val_accuracy: 0.6261 - val_loss: 1.2779\n",
      "Epoch 7/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9472 - loss: 0.1675 - val_accuracy: 0.6312 - val_loss: 1.4932\n",
      "Epoch 8/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9533 - loss: 0.1544 - val_accuracy: 0.6432 - val_loss: 1.4131\n",
      "Epoch 9/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9772 - loss: 0.0700 - val_accuracy: 0.6158 - val_loss: 2.1135\n",
      "Epoch 10/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9792 - loss: 0.0756 - val_accuracy: 0.6278 - val_loss: 1.7225\n",
      "Epoch 11/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9888 - loss: 0.0477 - val_accuracy: 0.6158 - val_loss: 2.2610\n",
      "Epoch 12/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9874 - loss: 0.0434 - val_accuracy: 0.6072 - val_loss: 1.6720\n",
      "Epoch 13/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9893 - loss: 0.0343 - val_accuracy: 0.6501 - val_loss: 1.9089\n",
      "Epoch 14/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9937 - loss: 0.0258 - val_accuracy: 0.6552 - val_loss: 1.9738\n",
      "Epoch 15/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9971 - loss: 0.0116 - val_accuracy: 0.6535 - val_loss: 2.1725\n",
      "Epoch 16/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9987 - loss: 0.0054 - val_accuracy: 0.6604 - val_loss: 2.2417\n",
      "Epoch 17/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9971 - loss: 0.0086 - val_accuracy: 0.6621 - val_loss: 1.9530\n",
      "Epoch 18/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9979 - loss: 0.0123 - val_accuracy: 0.6518 - val_loss: 2.1149\n",
      "Epoch 19/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9968 - loss: 0.0140 - val_accuracy: 0.6432 - val_loss: 2.5775\n",
      "Epoch 20/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9980 - loss: 0.0057 - val_accuracy: 0.6398 - val_loss: 2.5984\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "history_target = model_target.fit(\n",
    "    X_tr_seq, y_tr_target,\n",
    "    validation_data=(X_val_seq, y_val_target),\n",
    "    epochs=20,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation (Target)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "F1 Score (Target classification) [Val]: 0.6386058096606255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.68      0.65       117\n",
      "           1       0.63      0.49      0.55        79\n",
      "           2       0.61      0.72      0.66       135\n",
      "           3       0.74      0.62      0.68       125\n",
      "           4       0.63      0.62      0.62       127\n",
      "\n",
      "    accuracy                           0.64       583\n",
      "   macro avg       0.64      0.63      0.63       583\n",
      "weighted avg       0.64      0.64      0.64       583\n",
      "\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "F1 Score (Target classification) [Test]: 0.6338116733972252\n"
     ]
    }
   ],
   "source": [
    "# Validation set üzerinde\n",
    "val_preds_target = model_target.predict(X_val_seq).argmax(axis=1)\n",
    "print(\"F1 Score (Target classification) [Val]:\", f1_score(y_val_target, val_preds_target, average=\"weighted\"))\n",
    "print(classification_report(y_val_target, val_preds_target))\n",
    "\n",
    "# Test set üzerinde\n",
    "X_test_seq = text_to_seq(X_test_text)  # test verisine de preprocess\n",
    "test_preds_target = model_target.predict(X_test_seq).argmax(axis=1)\n",
    "print(\"F1 Score (Target classification) [Test]:\", \n",
    "      f1_score(y_test_target, test_preds_target, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model 2 - Stance Classification - Model Input: Tweet Text + Target Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\telat\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train aşamasında ground-truth target label kullanıyoruz\n",
    "target_onehot_train = to_categorical(y_tr_target, num_classes_target)\n",
    "target_onehot_val   = to_categorical(y_val_target, num_classes_target)\n",
    "\n",
    "# Tweet metni embed’ine ek veri olarak one-hot vektörü eklemek için\n",
    "# Keras’ta bir “functional API” model kuralım:\n",
    "tweet_input = layers.Input(shape=(MAX_SEQ_LEN,), name=\"tweet_input\")\n",
    "target_input = layers.Input(shape=(num_classes_target,), name=\"target_input\")\n",
    "\n",
    "# Tweet embeding + RNN\n",
    "embedding_layer = layers.Embedding(input_dim=MAX_VOCAB_SIZE, \n",
    "                                   output_dim=128, \n",
    "                                   input_length=MAX_SEQ_LEN)(tweet_input)\n",
    "lstm_layer = layers.LSTM(128, return_sequences=False)(embedding_layer)\n",
    "\n",
    "# RNN çıktısıyla target one-hot’u birleştir\n",
    "concat = layers.concatenate([lstm_layer, target_input])\n",
    "\n",
    "dense = layers.Dense(64, activation='relu')(concat)\n",
    "output = layers.Dense(num_classes_stance, activation='softmax')(dense)\n",
    "\n",
    "model_stance = keras.Model(inputs=[tweet_input, target_input], outputs=output)\n",
    "\n",
    "model_stance.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer= tf.keras.optimizers.Adam(0.008),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Traning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.4826 - loss: 1.0474 - val_accuracy: 0.5455 - val_loss: 0.9813\n",
      "Epoch 2/8\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5460 - loss: 0.9677 - val_accuracy: 0.5043 - val_loss: 0.9542\n",
      "Epoch 3/8\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.6550 - loss: 0.7690 - val_accuracy: 0.5472 - val_loss: 1.0700\n",
      "Epoch 4/8\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.7728 - loss: 0.4879 - val_accuracy: 0.5523 - val_loss: 1.1920\n",
      "Epoch 5/8\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8220 - loss: 0.3688 - val_accuracy: 0.5472 - val_loss: 1.5532\n",
      "Epoch 6/8\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.9274 - loss: 0.2071 - val_accuracy: 0.5763 - val_loss: 1.5917\n",
      "Epoch 7/8\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9542 - loss: 0.1392 - val_accuracy: 0.5592 - val_loss: 1.7636\n",
      "Epoch 8/8\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9915 - loss: 0.0401 - val_accuracy: 0.5437 - val_loss: 2.1887\n"
     ]
    }
   ],
   "source": [
    "history_stance = model_stance.fit(\n",
    "    [X_tr_seq, target_onehot_train],  # Girdi\n",
    "    y_tr_stance,                      # Çıktı stance label\n",
    "    validation_data=([X_val_seq, target_onehot_val], y_val_stance),\n",
    "    epochs=8,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation (Stance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "F1 Score (Stance classification) [Test]: 0.5527361859353696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.61      0.66       715\n",
      "           1       0.43      0.45      0.44       304\n",
      "           2       0.33      0.44      0.38       230\n",
      "\n",
      "    accuracy                           0.54      1249\n",
      "   macro avg       0.49      0.50      0.49      1249\n",
      "weighted avg       0.57      0.54      0.55      1249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Model–1 ile test verisindeki target tahmini:\n",
    "test_preds_target = model_target.predict(X_test_seq).argmax(axis=1)\n",
    "test_preds_target_onehot = to_categorical(test_preds_target, num_classes_target)\n",
    "\n",
    "# 2) Model–2 ile stance tahmini:\n",
    "test_preds_stance = model_stance.predict([X_test_seq, test_preds_target_onehot]).argmax(axis=1)\n",
    "\n",
    "# 3) Gerçek stance etiketleriyle kıyaslama:\n",
    "print(\"F1 Score (Stance classification) [Test]:\", \n",
    "      f1_score(y_test_stance, test_preds_stance, average=\"weighted\"))\n",
    "print(classification_report(y_test_stance, test_preds_stance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
