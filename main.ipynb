{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Installing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loading and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek: veriyi okuma\n",
    "df = pd.read_excel(\"Tweets.xlsx\")\n",
    "\n",
    "# Kolon isimlerinin aynen kaldığını varsayıyoruz:\n",
    "# [\"Tweet\", \"Target\", \"Train/Test\", \"Stance\", \"Opinion Toward\", \"Sentiment labels\"]\n",
    "\n",
    "# Temel text preprocessing\n",
    "def basic_preprocessing(text):\n",
    "    # Küçük harfe çevir\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Özel karakterleri temizle, sadece harf, rakam, boşluk, '-', '_', ve '#' karakterlerini bırak\n",
    "    text = re.sub(r\"[^\\w\\s#_-]+\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z0-9ığüşöçİĞÜŞÖÇ]+\", \" \", text)  # Noktalama ve özel karakterleri sil\n",
    "    text = text.strip()                          # Boşlukları (trim) temizle\n",
    "    return text\n",
    "\n",
    "df[\"Preprocessed_Tweet\"] = df[\"Tweet\"].apply(basic_preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Editing Category (Target) Classes and Stance (FAVOR / AGAINST / NEITHER) Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target için LabelEncoder\n",
    "le_target = LabelEncoder()\n",
    "df[\"Target_label\"] = le_target.fit_transform(df[\"Target\"])\n",
    "\n",
    "# Örnek olarak \"Atheism=0, Hillary Clinton=1, Feminist Mov=2, ...\" gibi dönüştürmüş olacak.\n",
    "num_classes_target = len(df[\"Target_label\"].unique())\n",
    "\n",
    "\n",
    "# Stance için LabelEncoder\n",
    "le_stance = LabelEncoder()\n",
    "df[\"Stance_label\"] = le_stance.fit_transform(df[\"Stance\"])\n",
    "num_classes_stance = len(df[\"Stance_label\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training/Validation Spliting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df[\"Train/Test\"] == \"Train\"].copy()\n",
    "df_test  = df[df[\"Train/Test\"] == \"Test\"].copy()\n",
    "\n",
    "X_train_text = df_train[\"Preprocessed_Tweet\"].values\n",
    "y_train_target = df_train[\"Target_label\"].values\n",
    "y_train_stance = df_train[\"Stance_label\"].values\n",
    "\n",
    "X_test_text = df_test[\"Preprocessed_Tweet\"].values\n",
    "y_test_target = df_test[\"Target_label\"].values\n",
    "y_test_stance = df_test[\"Stance_label\"].values\n",
    "\n",
    "\n",
    "X_tr_text, X_val_text, y_tr_target, y_val_target = train_test_split(\n",
    "    X_train_text, y_train_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_tr_text_s, X_val_text_s, y_tr_stance, y_val_stance = train_test_split(\n",
    "    X_train_text, y_train_stance, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenize and Sequence Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000   # Sözlük boyutu (isteğe göre artırılabilir)\n",
    "MAX_SEQ_LEN = 30         # Maksimum token sayısı\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_train[\"Preprocessed_Tweet\"])\n",
    "\n",
    "def text_to_seq(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
    "    return padded\n",
    "\n",
    "X_tr_seq = text_to_seq(X_tr_text)\n",
    "X_val_seq = text_to_seq(X_val_text)\n",
    "X_test_seq = text_to_seq(X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model–1: Target Classification - Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\telat\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_target = keras.models.Sequential([\n",
    "    layers.Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=128, \n",
    "                     input_length=MAX_SEQ_LEN),\n",
    "    layers.LSTM(64, return_sequences=False),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes_target, activation='softmax')\n",
    "])\n",
    "\n",
    "model_target.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Traninig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.2420 - loss: 1.6797 - val_accuracy: 0.2196 - val_loss: 1.6099\n",
      "Epoch 2/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.3225 - loss: 1.5411 - val_accuracy: 0.4254 - val_loss: 1.3174\n",
      "Epoch 3/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5606 - loss: 1.0497 - val_accuracy: 0.5455 - val_loss: 1.1579\n",
      "Epoch 4/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7541 - loss: 0.6330 - val_accuracy: 0.6158 - val_loss: 1.2200\n",
      "Epoch 5/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9129 - loss: 0.2919 - val_accuracy: 0.6449 - val_loss: 1.2845\n",
      "Epoch 6/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9649 - loss: 0.1489 - val_accuracy: 0.6484 - val_loss: 1.4652\n",
      "Epoch 7/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9745 - loss: 0.1096 - val_accuracy: 0.6535 - val_loss: 1.5320\n",
      "Epoch 8/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9916 - loss: 0.0443 - val_accuracy: 0.6724 - val_loss: 1.6614\n",
      "Epoch 9/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9929 - loss: 0.0361 - val_accuracy: 0.6638 - val_loss: 1.6819\n",
      "Epoch 10/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9947 - loss: 0.0288 - val_accuracy: 0.6707 - val_loss: 1.9501\n",
      "Epoch 11/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9931 - loss: 0.0354 - val_accuracy: 0.6535 - val_loss: 1.7344\n",
      "Epoch 12/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9826 - loss: 0.0601 - val_accuracy: 0.6364 - val_loss: 1.9012\n",
      "Epoch 13/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9813 - loss: 0.0951 - val_accuracy: 0.6329 - val_loss: 1.6230\n",
      "Epoch 14/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9891 - loss: 0.0413 - val_accuracy: 0.6690 - val_loss: 1.7156\n",
      "Epoch 15/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9965 - loss: 0.0160 - val_accuracy: 0.6587 - val_loss: 1.9568\n",
      "Epoch 16/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9962 - loss: 0.0236 - val_accuracy: 0.6655 - val_loss: 1.8212\n",
      "Epoch 17/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9985 - loss: 0.0140 - val_accuracy: 0.6775 - val_loss: 1.7823\n",
      "Epoch 18/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9991 - loss: 0.0055 - val_accuracy: 0.6827 - val_loss: 1.8781\n",
      "Epoch 19/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9971 - loss: 0.0134 - val_accuracy: 0.6895 - val_loss: 1.8867\n",
      "Epoch 20/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9977 - loss: 0.0161 - val_accuracy: 0.6913 - val_loss: 1.9159\n",
      "Epoch 21/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0127 - val_accuracy: 0.6792 - val_loss: 1.9267\n",
      "Epoch 22/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9981 - loss: 0.0101 - val_accuracy: 0.6690 - val_loss: 1.8807\n",
      "Epoch 23/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9998 - loss: 0.0021 - val_accuracy: 0.6690 - val_loss: 1.8863\n",
      "Epoch 24/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9981 - loss: 0.0079 - val_accuracy: 0.6741 - val_loss: 2.0690\n",
      "Epoch 25/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9996 - loss: 0.0025 - val_accuracy: 0.6758 - val_loss: 2.0896\n",
      "Epoch 26/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 0.6775 - val_loss: 2.1449\n",
      "Epoch 27/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9978 - loss: 0.0141 - val_accuracy: 0.6758 - val_loss: 2.3115\n",
      "Epoch 28/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9951 - loss: 0.0217 - val_accuracy: 0.6346 - val_loss: 2.3261\n",
      "Epoch 29/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9806 - loss: 0.0835 - val_accuracy: 0.6535 - val_loss: 1.8100\n",
      "Epoch 30/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9963 - loss: 0.0180 - val_accuracy: 0.6672 - val_loss: 2.1639\n",
      "Epoch 31/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9935 - loss: 0.0245 - val_accuracy: 0.6501 - val_loss: 1.8382\n",
      "Epoch 32/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9923 - loss: 0.0294 - val_accuracy: 0.6741 - val_loss: 1.7151\n",
      "Epoch 33/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9985 - loss: 0.0062 - val_accuracy: 0.6741 - val_loss: 1.8883\n",
      "Epoch 34/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9985 - loss: 0.0093 - val_accuracy: 0.6741 - val_loss: 1.9107\n",
      "Epoch 35/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9985 - loss: 0.0060 - val_accuracy: 0.6758 - val_loss: 1.8063\n",
      "Epoch 36/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9997 - loss: 0.0063 - val_accuracy: 0.6947 - val_loss: 1.8047\n",
      "Epoch 37/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9990 - loss: 0.0079 - val_accuracy: 0.6981 - val_loss: 1.8194\n",
      "Epoch 38/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 0.6827 - val_loss: 1.9633\n",
      "Epoch 39/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.6827 - val_loss: 1.9272\n",
      "Epoch 40/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9998 - loss: 0.0013 - val_accuracy: 0.6861 - val_loss: 1.9593\n",
      "Epoch 41/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 5.3909e-04 - val_accuracy: 0.6964 - val_loss: 2.0402\n",
      "Epoch 42/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.5561e-04 - val_accuracy: 0.6964 - val_loss: 2.0688\n",
      "Epoch 43/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.8867e-04 - val_accuracy: 0.6964 - val_loss: 2.0936\n",
      "Epoch 44/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.1564e-04 - val_accuracy: 0.6964 - val_loss: 2.1181\n",
      "Epoch 45/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.5609e-04 - val_accuracy: 0.6964 - val_loss: 2.1405\n",
      "Epoch 46/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.2918e-04 - val_accuracy: 0.6964 - val_loss: 2.1622\n",
      "Epoch 47/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.2596e-04 - val_accuracy: 0.6947 - val_loss: 2.1825\n",
      "Epoch 48/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.2041e-04 - val_accuracy: 0.6947 - val_loss: 2.2025\n",
      "Epoch 49/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.0735e-04 - val_accuracy: 0.6947 - val_loss: 2.2211\n",
      "Epoch 50/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.0161e-04 - val_accuracy: 0.6930 - val_loss: 2.2390\n",
      "Epoch 51/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 8.9010e-05 - val_accuracy: 0.6913 - val_loss: 2.2564\n",
      "Epoch 52/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 8.0751e-05 - val_accuracy: 0.6913 - val_loss: 2.2734\n",
      "Epoch 53/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 8.3096e-05 - val_accuracy: 0.6913 - val_loss: 2.2898\n",
      "Epoch 54/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 7.0277e-05 - val_accuracy: 0.6913 - val_loss: 2.3052\n",
      "Epoch 55/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 6.0402e-05 - val_accuracy: 0.6913 - val_loss: 2.3205\n",
      "Epoch 56/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 6.5360e-05 - val_accuracy: 0.6913 - val_loss: 2.3360\n",
      "Epoch 57/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 5.6854e-05 - val_accuracy: 0.6895 - val_loss: 2.3508\n",
      "Epoch 58/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 5.2926e-05 - val_accuracy: 0.6895 - val_loss: 2.3653\n",
      "Epoch 59/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.6422e-05 - val_accuracy: 0.6895 - val_loss: 2.3791\n",
      "Epoch 60/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.6324e-05 - val_accuracy: 0.6895 - val_loss: 2.3927\n",
      "Epoch 61/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.3100e-05 - val_accuracy: 0.6895 - val_loss: 2.4066\n",
      "Epoch 62/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.1763e-05 - val_accuracy: 0.6895 - val_loss: 2.4197\n",
      "Epoch 63/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.5718e-05 - val_accuracy: 0.6878 - val_loss: 2.4326\n",
      "Epoch 64/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.5096e-05 - val_accuracy: 0.6878 - val_loss: 2.4451\n",
      "Epoch 65/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.3533e-05 - val_accuracy: 0.6878 - val_loss: 2.4577\n",
      "Epoch 66/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.3218e-05 - val_accuracy: 0.6895 - val_loss: 2.4700\n",
      "Epoch 67/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.9080e-05 - val_accuracy: 0.6895 - val_loss: 2.4820\n",
      "Epoch 68/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 2.4294e-05 - val_accuracy: 0.6895 - val_loss: 2.4938\n",
      "Epoch 69/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.6361e-05 - val_accuracy: 0.6895 - val_loss: 2.5061\n",
      "Epoch 70/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.4522e-05 - val_accuracy: 0.6895 - val_loss: 2.5178\n",
      "Epoch 71/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.3306e-05 - val_accuracy: 0.6878 - val_loss: 2.5292\n",
      "Epoch 72/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.3659e-05 - val_accuracy: 0.6861 - val_loss: 2.5409\n",
      "Epoch 73/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.4407e-05 - val_accuracy: 0.6861 - val_loss: 2.5522\n",
      "Epoch 74/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.2857e-05 - val_accuracy: 0.6861 - val_loss: 2.5635\n",
      "Epoch 75/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.0066e-05 - val_accuracy: 0.6844 - val_loss: 2.5744\n",
      "Epoch 76/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.7127e-05 - val_accuracy: 0.6844 - val_loss: 2.5852\n",
      "Epoch 77/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.8236e-05 - val_accuracy: 0.6861 - val_loss: 2.5965\n",
      "Epoch 78/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.1613e-05 - val_accuracy: 0.6861 - val_loss: 2.6074\n",
      "Epoch 79/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.9687e-05 - val_accuracy: 0.6861 - val_loss: 2.6182\n",
      "Epoch 80/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.5045e-05 - val_accuracy: 0.6861 - val_loss: 2.6292\n",
      "Epoch 81/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.5708e-05 - val_accuracy: 0.6861 - val_loss: 2.6396\n",
      "Epoch 82/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.4167e-05 - val_accuracy: 0.6861 - val_loss: 2.6508\n",
      "Epoch 83/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.3082e-05 - val_accuracy: 0.6861 - val_loss: 2.6614\n",
      "Epoch 84/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.4078e-05 - val_accuracy: 0.6861 - val_loss: 2.6721\n",
      "Epoch 85/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.4589e-05 - val_accuracy: 0.6861 - val_loss: 2.6829\n",
      "Epoch 86/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.1074e-05 - val_accuracy: 0.6844 - val_loss: 2.6933\n",
      "Epoch 87/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.1348e-05 - val_accuracy: 0.6844 - val_loss: 2.7041\n",
      "Epoch 88/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.1998e-05 - val_accuracy: 0.6861 - val_loss: 2.7149\n",
      "Epoch 89/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 9.6868e-06 - val_accuracy: 0.6861 - val_loss: 2.7253\n",
      "Epoch 90/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 9.7765e-06 - val_accuracy: 0.6844 - val_loss: 2.7358\n",
      "Epoch 91/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.1018e-05 - val_accuracy: 0.6844 - val_loss: 2.7466\n",
      "Epoch 92/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 9.0406e-06 - val_accuracy: 0.6844 - val_loss: 2.7567\n",
      "Epoch 93/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 8.2453e-06 - val_accuracy: 0.6844 - val_loss: 2.7679\n",
      "Epoch 94/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 7.9271e-06 - val_accuracy: 0.6861 - val_loss: 2.7782\n",
      "Epoch 95/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 7.4100e-06 - val_accuracy: 0.6861 - val_loss: 2.7887\n",
      "Epoch 96/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 7.4648e-06 - val_accuracy: 0.6861 - val_loss: 2.7995\n",
      "Epoch 97/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 6.9324e-06 - val_accuracy: 0.6861 - val_loss: 2.8103\n",
      "Epoch 98/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 7.0203e-06 - val_accuracy: 0.6861 - val_loss: 2.8208\n",
      "Epoch 99/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 6.1271e-06 - val_accuracy: 0.6861 - val_loss: 2.8312\n",
      "Epoch 100/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 5.5969e-06 - val_accuracy: 0.6861 - val_loss: 2.8410\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "history_target = model_target.fit(\n",
    "    X_tr_seq, y_tr_target,\n",
    "    validation_data=(X_val_seq, y_val_target),\n",
    "    epochs=100,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation (Target)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "F1 Score (Target classification) [Val]: 0.687896412094745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68       117\n",
      "           1       0.54      0.59      0.57        79\n",
      "           3       0.70      0.73      0.72       135\n",
      "           4       0.82      0.70      0.76       125\n",
      "           5       0.68      0.66      0.67       127\n",
      "\n",
      "    accuracy                           0.69       583\n",
      "   macro avg       0.68      0.68      0.68       583\n",
      "weighted avg       0.69      0.69      0.69       583\n",
      "\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1 Score (Target classification) [Test]: 0.3261751542573221\n"
     ]
    }
   ],
   "source": [
    "# Validation set üzerinde\n",
    "val_preds_target = model_target.predict(X_val_seq).argmax(axis=1)\n",
    "print(\"F1 Score (Target classification) [Val]:\", f1_score(y_val_target, val_preds_target, average=\"weighted\"))\n",
    "print(classification_report(y_val_target, val_preds_target))\n",
    "\n",
    "# Test set üzerinde\n",
    "X_test_seq = text_to_seq(X_test_text)  # test verisine de preprocess\n",
    "test_preds_target = model_target.predict(X_test_seq).argmax(axis=1)\n",
    "print(\"F1 Score (Target classification) [Test]:\", \n",
    "      f1_score(y_test_target, test_preds_target, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model 2 - Stance Classification - Model Input: Tweet Text + Target Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\telat\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train aşamasında ground-truth target label kullanıyoruz\n",
    "target_onehot_train = to_categorical(y_tr_target, num_classes_target)\n",
    "target_onehot_val   = to_categorical(y_val_target, num_classes_target)\n",
    "\n",
    "# Tweet metni embed’ine ek veri olarak one-hot vektörü eklemek için\n",
    "# Keras’ta bir “functional API” model kuralım:\n",
    "tweet_input = layers.Input(shape=(MAX_SEQ_LEN,), name=\"tweet_input\")\n",
    "target_input = layers.Input(shape=(num_classes_target,), name=\"target_input\")\n",
    "\n",
    "# Tweet embeding + RNN\n",
    "embedding_layer = layers.Embedding(input_dim=MAX_VOCAB_SIZE, \n",
    "                                   output_dim=128, \n",
    "                                   input_length=MAX_SEQ_LEN)(tweet_input)\n",
    "lstm_layer = layers.LSTM(128, return_sequences=False)(embedding_layer)\n",
    "\n",
    "# RNN çıktısıyla target one-hot’u birleştir\n",
    "concat = layers.concatenate([lstm_layer, target_input])\n",
    "\n",
    "dense = layers.Dense(64, activation='relu')(concat)\n",
    "output = layers.Dense(num_classes_stance, activation='softmax')(dense)\n",
    "\n",
    "model_stance = keras.Model(inputs=[tweet_input, target_input], outputs=output)\n",
    "\n",
    "model_stance.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer= tf.keras.optimizers.SGD(0.01),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Traning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.3273 - loss: 1.1047 - val_accuracy: 0.5455 - val_loss: 1.0572\n",
      "Epoch 2/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5411 - loss: 1.0513 - val_accuracy: 0.5455 - val_loss: 1.0306\n",
      "Epoch 3/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5368 - loss: 1.0370 - val_accuracy: 0.5455 - val_loss: 1.0180\n",
      "Epoch 4/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5461 - loss: 1.0216 - val_accuracy: 0.5455 - val_loss: 1.0112\n",
      "Epoch 5/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5321 - loss: 1.0259 - val_accuracy: 0.5455 - val_loss: 1.0048\n",
      "Epoch 6/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5526 - loss: 1.0014 - val_accuracy: 0.5455 - val_loss: 1.0013\n",
      "Epoch 7/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5425 - loss: 1.0092 - val_accuracy: 0.5455 - val_loss: 0.9976\n",
      "Epoch 8/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5385 - loss: 1.0058 - val_accuracy: 0.5455 - val_loss: 0.9945\n",
      "Epoch 9/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5525 - loss: 0.9924 - val_accuracy: 0.5455 - val_loss: 0.9919\n",
      "Epoch 10/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5436 - loss: 0.9986 - val_accuracy: 0.5455 - val_loss: 0.9893\n",
      "Epoch 11/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5505 - loss: 0.9911 - val_accuracy: 0.5455 - val_loss: 0.9874\n",
      "Epoch 12/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5457 - loss: 0.9893 - val_accuracy: 0.5455 - val_loss: 0.9859\n",
      "Epoch 13/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5460 - loss: 0.9897 - val_accuracy: 0.5455 - val_loss: 0.9839\n",
      "Epoch 14/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5481 - loss: 0.9857 - val_accuracy: 0.5455 - val_loss: 0.9821\n",
      "Epoch 15/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5511 - loss: 0.9759 - val_accuracy: 0.5455 - val_loss: 0.9814\n",
      "Epoch 16/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5484 - loss: 0.9775 - val_accuracy: 0.5455 - val_loss: 0.9799\n",
      "Epoch 17/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5426 - loss: 0.9855 - val_accuracy: 0.5455 - val_loss: 0.9787\n",
      "Epoch 18/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5424 - loss: 0.9859 - val_accuracy: 0.5455 - val_loss: 0.9779\n",
      "Epoch 19/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5557 - loss: 0.9741 - val_accuracy: 0.5455 - val_loss: 0.9770\n",
      "Epoch 20/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5501 - loss: 0.9784 - val_accuracy: 0.5455 - val_loss: 0.9759\n",
      "Epoch 21/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5442 - loss: 0.9785 - val_accuracy: 0.5455 - val_loss: 0.9755\n",
      "Epoch 22/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5393 - loss: 0.9840 - val_accuracy: 0.5455 - val_loss: 0.9747\n",
      "Epoch 23/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5495 - loss: 0.9700 - val_accuracy: 0.5455 - val_loss: 0.9744\n",
      "Epoch 24/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5555 - loss: 0.9698 - val_accuracy: 0.5455 - val_loss: 0.9739\n",
      "Epoch 25/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5250 - loss: 0.9874 - val_accuracy: 0.5455 - val_loss: 0.9731\n",
      "Epoch 26/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5389 - loss: 0.9875 - val_accuracy: 0.5455 - val_loss: 0.9719\n",
      "Epoch 27/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5424 - loss: 0.9761 - val_accuracy: 0.5455 - val_loss: 0.9718\n",
      "Epoch 28/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5234 - loss: 0.9932 - val_accuracy: 0.5455 - val_loss: 0.9714\n",
      "Epoch 29/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5451 - loss: 0.9681 - val_accuracy: 0.5455 - val_loss: 0.9712\n",
      "Epoch 30/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5358 - loss: 0.9818 - val_accuracy: 0.5455 - val_loss: 0.9706\n",
      "Epoch 31/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5507 - loss: 0.9637 - val_accuracy: 0.5455 - val_loss: 0.9705\n",
      "Epoch 32/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5527 - loss: 0.9708 - val_accuracy: 0.5455 - val_loss: 0.9701\n",
      "Epoch 33/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5473 - loss: 0.9744 - val_accuracy: 0.5455 - val_loss: 0.9701\n",
      "Epoch 34/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5404 - loss: 0.9749 - val_accuracy: 0.5455 - val_loss: 0.9695\n",
      "Epoch 35/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5299 - loss: 0.9830 - val_accuracy: 0.5455 - val_loss: 0.9691\n",
      "Epoch 36/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5594 - loss: 0.9560 - val_accuracy: 0.5455 - val_loss: 0.9688\n",
      "Epoch 37/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5360 - loss: 0.9799 - val_accuracy: 0.5455 - val_loss: 0.9683\n",
      "Epoch 38/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5439 - loss: 0.9781 - val_accuracy: 0.5455 - val_loss: 0.9678\n",
      "Epoch 39/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5452 - loss: 0.9699 - val_accuracy: 0.5455 - val_loss: 0.9681\n",
      "Epoch 40/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5320 - loss: 0.9746 - val_accuracy: 0.5455 - val_loss: 0.9674\n",
      "Epoch 41/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5507 - loss: 0.9716 - val_accuracy: 0.5455 - val_loss: 0.9678\n",
      "Epoch 42/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5611 - loss: 0.9683 - val_accuracy: 0.5455 - val_loss: 0.9687\n",
      "Epoch 43/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5309 - loss: 0.9809 - val_accuracy: 0.5455 - val_loss: 0.9666\n",
      "Epoch 44/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5541 - loss: 0.9691 - val_accuracy: 0.5455 - val_loss: 0.9664\n",
      "Epoch 45/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5447 - loss: 0.9711 - val_accuracy: 0.5455 - val_loss: 0.9668\n",
      "Epoch 46/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5287 - loss: 0.9831 - val_accuracy: 0.5455 - val_loss: 0.9659\n",
      "Epoch 47/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5474 - loss: 0.9682 - val_accuracy: 0.5455 - val_loss: 0.9662\n",
      "Epoch 48/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5455 - loss: 0.9725 - val_accuracy: 0.5455 - val_loss: 0.9660\n",
      "Epoch 49/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5493 - loss: 0.9579 - val_accuracy: 0.5455 - val_loss: 0.9654\n",
      "Epoch 50/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5446 - loss: 0.9734 - val_accuracy: 0.5455 - val_loss: 0.9662\n",
      "Epoch 51/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5525 - loss: 0.9749 - val_accuracy: 0.5455 - val_loss: 0.9649\n",
      "Epoch 52/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5686 - loss: 0.9501 - val_accuracy: 0.5455 - val_loss: 0.9671\n",
      "Epoch 53/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5463 - loss: 0.9671 - val_accuracy: 0.5455 - val_loss: 0.9653\n",
      "Epoch 54/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5448 - loss: 0.9746 - val_accuracy: 0.5455 - val_loss: 0.9647\n",
      "Epoch 55/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5355 - loss: 0.9753 - val_accuracy: 0.5455 - val_loss: 0.9654\n",
      "Epoch 56/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5517 - loss: 0.9634 - val_accuracy: 0.5455 - val_loss: 0.9660\n",
      "Epoch 57/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5429 - loss: 0.9673 - val_accuracy: 0.5455 - val_loss: 0.9642\n",
      "Epoch 58/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5506 - loss: 0.9614 - val_accuracy: 0.5455 - val_loss: 0.9647\n",
      "Epoch 59/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5614 - loss: 0.9569 - val_accuracy: 0.5455 - val_loss: 0.9636\n",
      "Epoch 60/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5546 - loss: 0.9605 - val_accuracy: 0.5455 - val_loss: 0.9633\n",
      "Epoch 61/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5602 - loss: 0.9564 - val_accuracy: 0.5455 - val_loss: 0.9631\n",
      "Epoch 62/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5412 - loss: 0.9647 - val_accuracy: 0.5455 - val_loss: 0.9636\n",
      "Epoch 63/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5326 - loss: 0.9691 - val_accuracy: 0.5455 - val_loss: 0.9630\n",
      "Epoch 64/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5436 - loss: 0.9625 - val_accuracy: 0.5455 - val_loss: 0.9632\n",
      "Epoch 65/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5328 - loss: 0.9809 - val_accuracy: 0.5455 - val_loss: 0.9631\n",
      "Epoch 66/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5494 - loss: 0.9607 - val_accuracy: 0.5455 - val_loss: 0.9626\n",
      "Epoch 67/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5413 - loss: 0.9700 - val_accuracy: 0.5455 - val_loss: 0.9617\n",
      "Epoch 68/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5432 - loss: 0.9670 - val_accuracy: 0.5455 - val_loss: 0.9631\n",
      "Epoch 69/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5569 - loss: 0.9547 - val_accuracy: 0.5455 - val_loss: 0.9621\n",
      "Epoch 70/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5401 - loss: 0.9655 - val_accuracy: 0.5455 - val_loss: 0.9615\n",
      "Epoch 71/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5420 - loss: 0.9721 - val_accuracy: 0.5455 - val_loss: 0.9627\n",
      "Epoch 72/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5502 - loss: 0.9655 - val_accuracy: 0.5455 - val_loss: 0.9614\n",
      "Epoch 73/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5580 - loss: 0.9646 - val_accuracy: 0.5455 - val_loss: 0.9609\n",
      "Epoch 74/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5462 - loss: 0.9661 - val_accuracy: 0.5455 - val_loss: 0.9606\n",
      "Epoch 75/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5484 - loss: 0.9581 - val_accuracy: 0.5455 - val_loss: 0.9608\n",
      "Epoch 76/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5312 - loss: 0.9855 - val_accuracy: 0.5455 - val_loss: 0.9601\n",
      "Epoch 77/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5502 - loss: 0.9630 - val_accuracy: 0.5455 - val_loss: 0.9608\n",
      "Epoch 78/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5539 - loss: 0.9656 - val_accuracy: 0.5455 - val_loss: 0.9598\n",
      "Epoch 79/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5347 - loss: 0.9718 - val_accuracy: 0.5455 - val_loss: 0.9603\n",
      "Epoch 80/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5496 - loss: 0.9606 - val_accuracy: 0.5455 - val_loss: 0.9609\n",
      "Epoch 81/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5609 - loss: 0.9512 - val_accuracy: 0.5455 - val_loss: 0.9598\n",
      "Epoch 82/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5416 - loss: 0.9789 - val_accuracy: 0.5455 - val_loss: 0.9595\n",
      "Epoch 83/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5600 - loss: 0.9430 - val_accuracy: 0.5455 - val_loss: 0.9592\n",
      "Epoch 84/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5576 - loss: 0.9495 - val_accuracy: 0.5455 - val_loss: 0.9585\n",
      "Epoch 85/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5461 - loss: 0.9607 - val_accuracy: 0.5455 - val_loss: 0.9598\n",
      "Epoch 86/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5409 - loss: 0.9671 - val_accuracy: 0.5455 - val_loss: 0.9599\n",
      "Epoch 87/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5571 - loss: 0.9445 - val_accuracy: 0.5455 - val_loss: 0.9593\n",
      "Epoch 88/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5451 - loss: 0.9635 - val_accuracy: 0.5455 - val_loss: 0.9585\n",
      "Epoch 89/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5402 - loss: 0.9637 - val_accuracy: 0.5455 - val_loss: 0.9582\n",
      "Epoch 90/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5511 - loss: 0.9559 - val_accuracy: 0.5455 - val_loss: 0.9585\n",
      "Epoch 91/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5372 - loss: 0.9691 - val_accuracy: 0.5455 - val_loss: 0.9579\n",
      "Epoch 92/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5567 - loss: 0.9579 - val_accuracy: 0.5455 - val_loss: 0.9571\n",
      "Epoch 93/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5348 - loss: 0.9712 - val_accuracy: 0.5455 - val_loss: 0.9576\n",
      "Epoch 94/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5525 - loss: 0.9560 - val_accuracy: 0.5455 - val_loss: 0.9563\n",
      "Epoch 95/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5445 - loss: 0.9637 - val_accuracy: 0.5455 - val_loss: 0.9576\n",
      "Epoch 96/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5522 - loss: 0.9603 - val_accuracy: 0.5455 - val_loss: 0.9569\n",
      "Epoch 97/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5512 - loss: 0.9617 - val_accuracy: 0.5455 - val_loss: 0.9581\n",
      "Epoch 98/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5354 - loss: 0.9719 - val_accuracy: 0.5455 - val_loss: 0.9571\n",
      "Epoch 99/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5321 - loss: 0.9687 - val_accuracy: 0.5455 - val_loss: 0.9581\n",
      "Epoch 100/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5406 - loss: 0.9642 - val_accuracy: 0.5455 - val_loss: 0.9561\n"
     ]
    }
   ],
   "source": [
    "history_stance = model_stance.fit(\n",
    "    [X_tr_seq, target_onehot_train],  # Girdi\n",
    "    y_tr_stance,                      # Çıktı stance label\n",
    "    validation_data=([X_val_seq, target_onehot_val], y_val_stance),\n",
    "    epochs=100,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation (Stance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "F1 Score (Stance classification) [Test]: 0.4288603615692548\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.90      0.69      1014\n",
      "           1       0.38      0.26      0.31       452\n",
      "           2       0.22      0.00      0.01       490\n",
      "\n",
      "    accuracy                           0.53      1956\n",
      "   macro avg       0.38      0.39      0.33      1956\n",
      "weighted avg       0.43      0.53      0.43      1956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Model–1 ile test verisindeki target tahmini:\n",
    "test_preds_target = model_target.predict(X_test_seq).argmax(axis=1)\n",
    "test_preds_target_onehot = to_categorical(test_preds_target, num_classes_target)\n",
    "\n",
    "# 2) Model–2 ile stance tahmini:\n",
    "test_preds_stance = model_stance.predict([X_test_seq, test_preds_target_onehot]).argmax(axis=1)\n",
    "\n",
    "# 3) Gerçek stance etiketleriyle kıyaslama:\n",
    "print(\"F1 Score (Stance classification) [Test]:\", \n",
    "      f1_score(y_test_stance, test_preds_stance, average=\"weighted\"))\n",
    "print(classification_report(y_test_stance, test_preds_stance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
